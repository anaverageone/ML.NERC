{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3879322-59c6-4ae4-b7c8-31da5fdf58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "os.chdir(r'C:\\Users\\anaverageone\\htlt_env\\Machine_Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2737f40a-cc59-4579-9c43-24fab3a806dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file= 'data/conll2003.train.conll'\n",
    "dev_file = 'data/conll2003.dev.conll'\n",
    "test_file = 'data/conll2003.test.conll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c820b4f-a9cb-4aec-9d72-9e07164ca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dc106e-5485-41fd-9bfc-7e3080f5f873",
   "metadata": {},
   "source": [
    "**Using word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b396605-19f7-4f7b-9dfd-20c754410971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    lr_classifier = LogisticRegression(max_iter=10000)\n",
    "    # lr_classifier = LogisticRegression(solver='saga', max_iter=10000)\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "\n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892d951-bf05-47da-83d8-11238b924820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Classifying data...\n",
      "Running evaluation...\n",
      "CONFUSION MATRIX\n",
      "[[ 1516    17   128    12    26     0    23    23    92]\n",
      " [   51   650    33    21     4    16    14     2   131]\n",
      " [  122    36   885    31     8     4    77    46   132]\n",
      " [   29    10    20  1308     2     1     5   351   116]\n",
      " [   30     0     7     7   140     4    36    10    23]\n",
      " [    3    38    12     3    10   152     9    12   107]\n",
      " [   46    14    96    26    39    14   311    19   186]\n",
      " [   19     5    29   262     1     3    15   755   218]\n",
      " [    9    41    74     5     5    17    41     5 42562]]\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC  0.83068493 0.82525857 0.82796286      1837\n",
      "      B-MISC  0.80147965 0.70498915 0.75014426       922\n",
      "       B-ORG  0.68925234 0.65995526 0.67428571      1341\n",
      "       B-PER  0.78089552 0.71009772 0.74381575      1842\n",
      "       I-LOC  0.59574468 0.54474708 0.56910569       257\n",
      "      I-MISC  0.72037915 0.43930636 0.54578097       346\n",
      "       I-ORG  0.58568738 0.41411451 0.48517941       751\n",
      "       I-PER  0.61733442 0.57765876 0.59683794      1307\n",
      "           O  0.97693208 0.99539278 0.98607604     42759\n",
      "\n",
      "    accuracy                      0.93997508     51362\n",
      "   macro avg  0.73315446 0.65239113 0.68657652     51362\n",
      "weighted avg  0.93550355 0.93997508 0.93707584     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing announcements of where the code is at (since some of these steps take a while)\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(train_file,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Classifying data...')\n",
    "predicted, gold = label_data_using_word_embeddings(dev_file, word_embedding_model, classifier)\n",
    "print('Running evaluation...')\n",
    "print('CONFUSION MATRIX')\n",
    "cf_matrix = confusion_matrix(gold, predicted)\n",
    "print(cf_matrix)\n",
    "print('CLASSIFICATION REPORT')\n",
    "report = classification_report(gold,predicted,digits = 8)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e68665-1c50-4f42-b206-0ea18f7961ac",
   "metadata": {},
   "source": [
    "**Including preceding token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "713ff9fd-d601-4687-a1c9-8f49cbf62ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(  row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2b9c8e3-450d-44cf-9f13-c648e0edb93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier...\n",
      "Classifying data...\n",
      "Running evaluation...\n",
      "CONFUSION MATRIX\n",
      "[[ 1516    21   128    11    27     0    31    26    77]\n",
      " [   41   668    29    16     4    16    18     9   121]\n",
      " [  117    36   892    35    14     9    84    47   107]\n",
      " [   30     8    22  1353     2     1     5   315   106]\n",
      " [   31     0     7     8   144     2    35     8    22]\n",
      " [    5    39    13     3    10   156    10    12    98]\n",
      " [   49    15   102    25    45    14   307    19   175]\n",
      " [   18     4    30   253     1     3    12   786   200]\n",
      " [    7    37    56     5     4    17    28     8 42597]]\n",
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC  0.83572216 0.82525857 0.83045741      1837\n",
      "      B-MISC  0.80676329 0.72451193 0.76342857       922\n",
      "       B-ORG  0.69741986 0.66517524 0.68091603      1341\n",
      "       B-PER  0.79169105 0.73452769 0.76203886      1842\n",
      "       I-LOC  0.57370518 0.56031128 0.56692913       257\n",
      "      I-MISC  0.71559633 0.45086705 0.55319149       346\n",
      "       I-ORG  0.57924528 0.40878828 0.47931304       751\n",
      "       I-PER  0.63902439 0.60137720 0.61962948      1307\n",
      "           O  0.97917385 0.99621132 0.98761911     42759\n",
      "\n",
      "    accuracy                      0.94270083     51362\n",
      "   macro avg  0.73537127 0.66300318 0.69372479     51362\n",
      "weighted avg  0.93856048 0.94270083 0.94004801     51362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(train_file,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Classifying data...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(dev_file, word_embedding_model, classifier)\n",
    "print('Running evaluation...')\n",
    "print('CONFUSION MATRIX')\n",
    "cf_matrix = confusion_matrix(gold, predicted)\n",
    "print(cf_matrix)\n",
    "print('CLASSIFICATION REPORT')\n",
    "report = classification_report(gold,predicted,digits = 8)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10eb7b-2aa1-4689-9e55-f25bb206842b",
   "metadata": {},
   "source": [
    "## A mixed system\n",
    "\n",
    "The code below combines traditional features with word embeddings. Note that we only include features with a limited range of possible values. Combining one-hot token representations (using highly sparse dimensions) with dense representations is generally not a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66b47b77-7b87-4ca0-8115-0ff4c816aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    sparse_vectors.reshape(-1, 1)\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['pos','chunk'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "feature_to_index = {'token': 0, 'pos': 1, 'chunk': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57120a69-1e19-466c-9562-15e76ea3f43f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(train_file, word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27cf318c-33ff-4ed3-8155-c92cb3aef0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203621\n",
      "203621\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_vectors))\n",
    "print(len(gold_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96eee570-7485-41db-b2cd-8b84a2f1b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier....\n"
     ]
    }
   ],
   "source": [
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73e7d3c9-cd58-4817-9dd5-5c65a4b26070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassifying data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m predicted, gold \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_data_using_word_embeddings_current_and_preceding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_classifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36mlabel_data_using_word_embeddings_current_and_preceding\u001b[1;34m(testfile, word_embedding_model, classifier)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03mFunction that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m:return labels: list of gold labels\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    171\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n\u001b[1;32m--> 172\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, labels\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\_base.py:309\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m        Predicted class label per sample.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    311\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\_base.py:284\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    class would be predicted.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    282\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 284\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m n_features:\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[0;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[1;32mD:\\ANACONDA\\lib\\site-packages\\sklearn\\utils\\validation.py:694\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 694\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    695\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    696\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    697\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    698\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array))\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "print('Classifying data...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(dev_file, word_embedding_model, lr_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d348f-441c-4c24-a2b1-fde8003ee18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running evaluation...')\n",
    "print('CONFUSION MATRIX')\n",
    "cf_matrix = confusion_matrix(gold, predicted)\n",
    "print(cf_matrix)\n",
    "print('CLASSIFICATION REPORT')\n",
    "report = classification_report(gold,predicted,digits = 8)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
