{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2deed5b1-77fa-4c3a-8ad0-9a4973c94039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anaverageone\\htlt_env\\Machine_Learning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import gensim \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "os.chdir(r'C:\\Users\\anaverageone\\htlt_env\\Machine_Learning')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639f2cfe-7cfe-4c94-8b1d-7a0970852609",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('models/GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "febede47-594f-4db5-a8cd-c0d90d38650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file= 'data/conll2003.train.conll'\n",
    "test_file = 'data/conll2003.test.conll'\n",
    "with open(train_file) as f:\n",
    "    train_cap = f.read() + '\\n'\n",
    "with open(test_file) as f:\n",
    "    test_cap = f.read() + '\\n'\n",
    "train_text, test_text = open(\"data/conll_train_cap.txt\", \"w\"), open(\"data/conll_test_cap.txt\", \"w\")\n",
    "train_text.write(train_cap)\n",
    "test_text.write(test_cap)\n",
    "train_text.close()\n",
    "test_text.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b1a1a19-bbe5-41d0-96d7-effac16ada3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = 'data/conll_train_cap.txt'\n",
    "testfile = 'data/conll_test_cap.txt'\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    \n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 4:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer). \n",
    "            labels.append(row[-1])\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "    \n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector \n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "    \n",
    "    return lr_classifier, vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e2a997-0eed-43c4-b277-3fcb70f05ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features and labels:\n",
    "feature_values, labels = extract_features_token_only_and_labels(trainfile) \n",
    "#create vectorizer and trained classifier:\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c95110b-61c5-4b73-91fa-4c04d1082f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "     \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1842642-899a-417d-bc15-0ef45e1ffa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5e9db7-c7fe-43b7-9cd7-ae6d358f0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n",
    "#vectorizer and lr_classifier are the vectorizer and classifiers created in the previous cell.\n",
    "#it is important that the same vectorizer is used for both training and testing: they should use the same mapping from values to dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f432e7-efd7-4d06-95cb-c0c460330b85",
   "metadata": {},
   "source": [
    "**Using word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76e77d1e-f5f0-4c28-91ac-9d20f1a8f1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('../models/GoogleNews-vectors-negative300.bin.gz', binary=True)  \n",
    "\n",
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "\n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3ae018c-f786-4232-b8b7-889c87b4780f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n"
     ]
    }
   ],
   "source": [
    "# I printing announcements of where the code is at (since some of these steps take a while)\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Running evaluation...')\n",
    "predictions, gold = label_data_using_word_embeddings(testfile, word_embedding_model, classifier)\n",
    "# print_confusion_matrix(predictions, goldlabels)\n",
    "# print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80f93444-675e-4465-ad77-c2e5149a10c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.77      0.82      0.79      1668\n",
      "      B-MISC       0.71      0.69      0.70       702\n",
      "       B-ORG       0.69      0.66      0.67      1661\n",
      "       B-PER       0.74      0.67      0.70      1617\n",
      "       I-LOC       0.54      0.48      0.51       257\n",
      "      I-MISC       0.55      0.54      0.54       216\n",
      "       I-ORG       0.51      0.39      0.44       835\n",
      "       I-PER       0.57      0.49      0.53      1156\n",
      "           O       0.98      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.67      0.63      0.65     46435\n",
      "weighted avg       0.92      0.93      0.93     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(gold, predictions, zero_division=0)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c8abf-e99f-41bb-bb09-54845992a7f2",
   "metadata": {},
   "source": [
    "**Including preceding token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7863b6b4-1f42-4530-b8ea-d21484fc0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            if row[0] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[0]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(row[-1])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f7923d-af2c-488b-b943-f0bd76e43f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier...\n",
      "Running evaluation...\n"
     ]
    }
   ],
   "source": [
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(trainfile,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier)\n",
    "# print_confusion_matrix(predictions, goldlabels)\n",
    "# print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3a90ec2-3d45-43ff-8ce6-8f8779055d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.77      0.82      0.79      1668\n",
      "      B-MISC       0.71      0.69      0.70       702\n",
      "       B-ORG       0.69      0.66      0.67      1661\n",
      "       B-PER       0.74      0.67      0.70      1617\n",
      "       I-LOC       0.54      0.48      0.51       257\n",
      "      I-MISC       0.55      0.54      0.54       216\n",
      "       I-ORG       0.51      0.39      0.44       835\n",
      "       I-PER       0.57      0.49      0.53      1156\n",
      "           O       0.98      0.99      0.98     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.67      0.63      0.65     46435\n",
      "weighted avg       0.92      0.93      0.93     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(gold, predictions, zero_division=0)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c02da-bb96-4a9f-92e5-0fd4b55afb7c",
   "metadata": {},
   "source": [
    "**A mixed system**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b51e584-9a7e-4efb-b8c9-ec11952be2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = ['pos','chunk']\n",
    "feature_to_index = {'Token': 0, 'pos': 1, 'chunk': 2}\n",
    "\n",
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 4:\n",
    "            token_vector = extract_word_embedding(row[0], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['pos','chunk'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[-1])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21bf49a1-558e-4b4e-b798-cacc46a5d1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n"
     ]
    }
   ],
   "source": [
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(trainfile, word_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6acdcc8-2a69-406a-978b-b0c1ba4263c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the evaluation...\n"
     ]
    }
   ],
   "source": [
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(testfile, lr_classifier, vectorizer, word_embedding_model)\n",
    "# print_confusion_matrix(predictions, goldlabels)\n",
    "# print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "047192db-14e1-4abf-a1eb-e3ea264ec4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.77      0.81      0.79      1668\n",
      "      B-MISC       0.71      0.69      0.70       702\n",
      "       B-ORG       0.70      0.65      0.67      1661\n",
      "       B-PER       0.86      0.78      0.82      1617\n",
      "       I-LOC       0.51      0.47      0.49       257\n",
      "      I-MISC       0.56      0.58      0.57       216\n",
      "       I-ORG       0.57      0.48      0.52       835\n",
      "       I-PER       0.71      0.81      0.76      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.94     46435\n",
      "   macro avg       0.71      0.70      0.70     46435\n",
      "weighted avg       0.94      0.94      0.94     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(goldlabels, predictions, zero_division=0)\n",
    "print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fb6b6-022e-432e-8074-d60e801a9622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad71d66-2340-469e-b0e8-2b8f160fdf86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
